# -*- coding: utf-8 -*-
"""explore_admission_notes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/somnathrakshit/d3dd0d3bc61f5774f82860e4f9e4f3ee/explore_admission_notes.ipynb
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# https://towardsdatascience.com/introduction-to-clinical-natural-language-processing-predicting-hospital-readmission-with-1736d52bc709

import numpy as np
# !kill -9 -1
import pandas as pd
from nltk import word_tokenize
nltk.download('punkt')
import string
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import *
import matplotlib.pyplot as plt
from sklearn.pipeline import make_pipeline
import nltk
from lime.lime_text import LimeTextExplainer
from joblib import dump, load
import base64


# !nvidia-smi

# nltk.download('all')


def firstWork():
    df_adm = pd.read_csv('./server/ADMISSIONS.csv.gz')

    print("Made it passsed reading Admissions")
    # convert to dates
    df_adm.ADMITTIME = pd.to_datetime(df_adm.ADMITTIME, format='%Y-%m-%d %H:%M:%S', errors='coerce')
    df_adm.DISCHTIME = pd.to_datetime(df_adm.DISCHTIME, format='%Y-%m-%d %H:%M:%S', errors='coerce')
    df_adm.DEATHTIME = pd.to_datetime(df_adm.DEATHTIME, format='%Y-%m-%d %H:%M:%S', errors='coerce')

    # sort by subject_ID and admission date
    df_adm = df_adm.sort_values(['SUBJECT_ID', 'ADMITTIME'])
    df_adm = df_adm.reset_index(drop=True)
    df_adm.head()

    # add the next admission date and type for each subject using groupby
    # you have to use groupby otherwise the dates will be from different subjects
    df_adm['NEXT_ADMITTIME'] = df_adm.groupby('SUBJECT_ID').ADMITTIME.shift(-1)  # get the next admission type
    df_adm['NEXT_ADMISSION_TYPE'] = df_adm.groupby('SUBJECT_ID').ADMISSION_TYPE.shift(-1)
    df_adm.head()

    # get rows where next admission is elective and replace with naT or nan
    rows = df_adm.NEXT_ADMISSION_TYPE == 'ELECTIVE'
    df_adm.loc[rows, 'NEXT_ADMITTIME'] = pd.NaT
    df_adm.loc[rows, 'NEXT_ADMISSION_TYPE'] = np.NaN
    df_adm.head()

    # sort by subject_ID and admission date
    # it is safer to sort right before the fill in case something changed the order above
    df_adm = df_adm.sort_values(['SUBJECT_ID', 'ADMITTIME'])  # back fill (this will take a little while)
    df_adm[['NEXT_ADMITTIME', 'NEXT_ADMISSION_TYPE']] = df_adm.groupby(['SUBJECT_ID'])[
        ['NEXT_ADMITTIME', 'NEXT_ADMISSION_TYPE']].fillna(method='bfill')
    df_adm.head()

    df_adm['DAYS_NEXT_ADMIT'] = (df_adm.NEXT_ADMITTIME - df_adm.DISCHTIME).dt.total_seconds() / (24 * 60 * 60)
    df_adm.head()

    df_adm['DAYS_NEXT_ADMIT'].hist(bins=10)
    return df_adm


def secondPart(df_adm):
    df_notes = pd.read_csv("./server/NOTEEVENTS.csv.gz", low_memory=False)
    # Since the next step is to merge the notes on the admissions table, we might
    # have the assumption that there is one discharge summary per admission, but
    # we should probably check this. We can check this with an assert statement, which ends up failing.

    # filter to discharge summary
    df_notes_dis_sum = df_notes.loc[df_notes.CATEGORY == 'Discharge summary']

    df_notes_dis_sum_last = (df_notes_dis_sum.groupby(['SUBJECT_ID', 'HADM_ID']).nth(-1)).reset_index()
    assert df_notes_dis_sum_last.duplicated(['HADM_ID']).sum() == 0, 'Multiple discharge summaries per admission'

    df_adm_notes = pd.merge(
        df_adm[['SUBJECT_ID', 'HADM_ID', 'ADMITTIME', 'DISCHTIME', 'DAYS_NEXT_ADMIT', 'NEXT_ADMITTIME',
                'ADMISSION_TYPE', 'DEATHTIME']],
        df_notes_dis_sum_last[['SUBJECT_ID', 'HADM_ID', 'TEXT']],
        on=['SUBJECT_ID', 'HADM_ID'],
        how='left')
    assert len(df_adm) == len(df_adm_notes), 'Number of rows increased'

    df_adm_notes.groupby('ADMISSION_TYPE').apply(lambda g: g.TEXT.isnull().sum()) / df_adm_notes.groupby(
        'ADMISSION_TYPE').size()

    df_adm_notes['OUTPUT_LABEL'] = (df_adm_notes.DAYS_NEXT_ADMIT < 30).astype('int')

    df_adm_notes_clean = df_adm_notes
    return df_adm_notes_clean


def thirdshuffle(df_adm_notes_clean):
    # shuffle the samples
    df_adm_notes_clean = df_adm_notes_clean.sample(n=len(df_adm_notes_clean), random_state=42)
    df_adm_notes_clean = df_adm_notes_clean.reset_index(drop=True)  # Save 30% of the data as validation and test data
    df_valid_test = df_adm_notes_clean.sample(frac=0.30, random_state=42)
    df_test = df_valid_test.sample(frac=0.5, random_state=42)
    df_valid = df_valid_test.drop(df_test.index)  # use the rest of the data as training data
    df_train_all = df_adm_notes_clean.drop(df_valid_test.index)

    # split the training data into positive and negative
    rows_pos = df_train_all.OUTPUT_LABEL == 1
    df_train_pos = df_train_all.loc[rows_pos]
    df_train_neg = df_train_all.loc[~rows_pos]  # merge the balanced data
    df_train = pd.concat([df_train_pos, df_train_neg.sample(n=len(df_train_pos), random_state=42)],
                         axis=0)  # shuffle the order of training samples
    df_train = df_train.sample(n=len(df_train), random_state=42).reset_index(drop=True)
    # preprocess the text to deal with known issues
    df_train = preprocess_text(df_train)
    df_valid = preprocess_text(df_valid)
    df_test = preprocess_text(df_test)
    # for key in df_valid.TEXT:
    #     print(df_valid.TEXT[38143])
    # print(df_valid)
    return (df_train, df_test, df_valid)


def preprocess_text(df):
    # This function preprocesses the text by filling not a number and replacing new lines ('\n') and carriage returns
    # ('\r')
    df.TEXT = df.TEXT.fillna(' ')
    df.TEXT = df.TEXT.str.replace('\n', ' ')
    df.TEXT = df.TEXT.str.replace('\r', ' ')
    return df


def tokenizer_better(text):
    # tokenize the text by replacing punctuation and numbers with spaces and lowercase all words
    punc_list = string.punctuation + '0123456789'
    t = str.maketrans(dict.fromkeys(punc_list, " "))
    text = text.lower().translate(t)
    tokens = word_tokenize(text)
    return tokens


def forthCountVectro(df_train, df_valid):
    word_tokenize('This should be tokenized. 02/02/2018 sentence has stars**')
    sample_text = ['Data science is about the data', 'The science is amazing',
                   'Predictive modeling is part of data science']

    vect = CountVectorizer(tokenizer=tokenizer_better)
    vect.fit(sample_text)  # matrix is stored as a sparse matrix (since you have a lot of zeros)
    X = vect.transform(sample_text)

    vect = CountVectorizer(tokenizer=tokenizer_better)
    vect.fit(sample_text)  # matrix is stored as a sparse matrix (since you have a lot of zeros)
    X = vect.transform(sample_text)

    # fit our vectorizer. This will take a while depending on your computer. from sklearn.feature_extraction.text import CountVectorizer
    vect = CountVectorizer(max_features=3000, tokenizer=tokenizer_better)  # this could take a while
    vect.fit(df_train.TEXT.values)

    my_stop_words = ['the', 'and', 'to', 'of', 'was', 'with', 'a', 'on', 'in', 'for', 'name',
                     'is', 'patient', 's', 'he', 'at', 'as', 'or', 'one', 'she', 'his', 'her', 'am',
                     'were', 'you', 'pt', 'pm', 'by', 'be', 'had', 'your', 'this', 'date',
                     'from', 'there', 'an', 'that', 'p', 'are', 'have', 'has', 'h', 'but', 'o',
                     'namepattern', 'which', 'every', 'also']

    vect = CountVectorizer(max_features=3000,
                           tokenizer=tokenizer_better,
                           stop_words=my_stop_words)
    # this could take a while
    vect.fit(df_train.TEXT.values)

    X_train_tf = vect.transform(df_train.TEXT.values)
    X_valid_tf = vect.transform(df_valid.TEXT.values)

    y_train = df_train.OUTPUT_LABEL
    y_valid = df_valid.OUTPUT_LABEL
    return (X_train_tf, X_valid_tf, y_train, y_valid, vect)


def fifthForestClassifier(X_train_tf, X_valid_tf, y_train, y_valid):
    # logistic regression

    clf = RandomForestClassifier(n_estimators=100)
    clf.fit(X_train_tf, y_train)

    model = clf
    y_train_preds = model.predict_proba(X_train_tf)[:, 1]
    y_valid_preds = model.predict_proba(X_valid_tf)[:, 1]

    y_train_lab = model.predict(X_train_tf)
    y_valid_lab = model.predict(X_valid_tf)

    print(y_valid_lab[:15])

    conf_matrix = confusion_matrix(y_valid, y_valid_lab)
    print(conf_matrix)

    accuracy = accuracy_score(y_valid, y_valid_lab)
    print(accuracy)

    recall = recall_score(y_valid, y_valid_lab)
    print(recall)

    fpr_RF, tpr_RF, thresholds_RF = roc_curve(y_valid, y_valid_lab)
    fpr_LR, tpr_LR, thresholds_LR = roc_curve(y_valid, y_valid_lab)

    auc_RF = roc_auc_score(y_valid, y_valid_lab)
    auc_LR = roc_auc_score(y_valid, y_valid_lab)

    plt.plot(fpr_RF, tpr_RF, 'r-', label='RF AUC: %.3f' % auc_RF)
    plt.plot(fpr_LR, tpr_LR, 'b-', label='LR AUC: %.3f' % auc_LR)
    plt.plot([0, 1], [0, 1], 'k-', label='random')
    plt.plot([0, 0, 1, 1], [0, 1, 1, 1], 'g-', label='perfect')
    plt.legend()
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.show()
    return model


def sixthProbability(df_train, df_valid, vect, model):
    c = make_pipeline(vect, model)

    print(c.predict_proba([df_train.TEXT[0]]))
    print("start c ")

    # dump(c, 'model.joblib')
    # print("Done c ")
    # dump(model,'model_model.joblib')
    # print("Done model ")
    # dump(vect,'vert.joblib')
    # print("Done vect ")
    #
    # class_name = ['0', '1']
    # explainer = LimeTextExplainer(class_names=class_name)
    # print("Length of number of text we can try from 0 to:",len(df_valid.TEXT))
    # df_valid.head()

    # while(True):
    #     try:
    #         idx = int(input("Who give us id number"))
    #         # idx = 38143
    #         exp = explainer.explain_instance(df_valid.TEXT[idx], c.predict_proba, num_features=6)
    #         print('Document id: %d' % idx)
    #         prob = c.predict_proba([df_valid.TEXT[idx]])[0, 1]
    #         print('Probability(0) =', prob)
    #     except Exception as a:
    #         pass
    # print('True class: %s' % class_names[df_valid.TEXT[idx]])

    # fig = exp.as_pyplot_figure()
    #
    # exp.show_in_notebook(text=True)
    return c


def runItAll():
    print("We have Started")
    df_adm = firstWork()
    print("finished first work")
    df_adm_notes_clean = secondPart(df_adm)
    print("finished second work")
    df_train, df_test, df_valid = thirdshuffle(df_adm_notes_clean)
    print("finished Third work")
    X_train_tf, X_valid_tf, y_train, y_valid, vect = forthCountVectro(df_train, df_valid)
    print("finished fourth work")
    model = fifthForestClassifier(X_train_tf, X_valid_tf, y_train, y_valid)
    print("finished fifth work")
    c = sixthProbability(df_train, df_valid, vect, model)
    return c


def processString(fixString):
    fixString = base64.b64decode(fixString)
    fixString = str(fixString).replace('b\"', '')
    fixString = fixString.replace('b\'', '')
    fixString = fixString.replace('\\n', ' ')
    fixString = fixString.replace('\n', ' ')
    fixString = fixString.replace('\r', ' ')
    return fixString


# def predictModTester(model):
#     firstPerson = "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"
#     firstPerson = processString(firstPerson)
#     secondPerson = "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"
#     secondPerson = processString(secondPerson)
#     person4 = "RlVMTCBDT0RFICAgICAgICAgIFVuaXZlcnNhbCBQcmVjYXV0aW9ucwpBbGxlcmd5OiAgSGVwYXJpbiAgIChISVQpCgoKNzAgeXItb2xkIGZlbWFsZSBhZG0gZnJvbSBFQ0Ygdy8gZmV2ZXIvY2hpbGxzLiAgU2hlIHdhcyByZWNlbnRseSBkL2MnZCBmcm9tIFsqKkhvc3BpdGFsMSA0MSoqXSBhZnRlciA0IGRheXMgbWFuYWdlbWVudCBvZiBhYmQgcGFpbiwgZmV2ZXIgYW5kIHN1c3BlY3RlZCBwZXJpdG9uaXRpcy4gIFNoZSBwcmVzZW50ZWQgdG8gdGhlIEVSIGF0IDQ6MzBwbSB3LyBmZXZlci9jaGlsbHMgYWZ0ZXIgaGVyIGRpYWx5c2lzIHRyZWF0bWVudCB0b2RheS4gIEluIEVSLCBUPTEwMyBvcmFsLCBMYWN0YXRlPTQuNC4gIFBsYWNlZCBvbiBNVVNUIHByb3RvY29sLCBidXQgTURzIHVuYWJsZSB0byBwbGFjZSBzZXBzaXMgY2F0aCBhZnRlciBzZXZlcmFsIGF0dGVtcHRzIGFuZCBldmVuIHVuZGVyIHUvcy4gIFNoZSBoYXMgUElWICMxOCBhbmQgUiBjaGVzdCBkaWFseXNpcyBjYXRoLiAgU2hlIHJlY2VpdmVkIENlZnRyaWF4b25lLCBWYW5jbyBhbmQgRmxhZ3lsIGluIEVSLiAgSFI9MTAwLTExMHMsIEJQPTEyOC81OS4gIDAyc2F0IDk5JSBvbiA0TC4gIFQgZG93biB0byAxMDAuOCBhZnRlciB0eWxlbm9sIGFuZCBsYXN0IGxhY3RhdGU9IDMuMyBwcmlvciB0byBhZG0gdG8gTUlDVS4KClBNSDogIEhJVCwgTklERE0sIEVTUkQgb24gZGlhbHlzaXMgMy93ZWVrIChoYWQgYmVlbiBvbiBQRCksIGRpdmVydGljdWxpdGlzLCBHSUIsIGJsZWVkaW5nIHVsY2VycywgQ0hGLCBIVE4sIGdvdXQsIGFuZW1pYSwgYW9ydGljIGluc3VmZmljaWVuY3ksIEVGPTQzJSwgYWZpYiwgbWVudGFsIHN0YXR1cyBjaGFuZ2VzLCBDLWRpZiBjb2xpdGlzLCBsYW1pbmVjdG9teSwgYy1zZWN0aW9uIHg0LCBjaG9sZWN5c3RlY3RvbXk="
#     person4 = processString(person4)
#     setPeople = [firstPerson, secondPerson, person4]
#     print(setPeople[0])
#     print(setPeople[1])
#     print(setPeople[2])
#     while True:
#         try:
#             words = input("Who give us text")
#             prob = model.predict_proba([setPeople[int(words)]])[0, 1]
#             print('Probability(0) =', prob)
#         except Exception as a:
#             print(a)
#             pass
#

def predict(model, note):
    note = processString(note)
    prob = model.predict_proba([note])[0, 1]
    return prob


def simpleDownloadFixes():
    import ssl
    try:
        _create_unverified_https_context = ssl._create_unverified_context
    except AttributeError:
        pass
    else:
        ssl._create_default_https_context = _create_unverified_https_context
    nltk.download('punkt')


def doIT():
    modelName = "./server/model.joblib"
    model = ""
    try:
        model = load(modelName)
        # predictModTester(model)
    except Exception as e:
        print("failed to Read model", e)
        model = runItAll()
        dump(model, modelName)
        pass
    return model
